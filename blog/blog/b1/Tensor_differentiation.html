<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

    <title>ranjan's blog - Random assortment of things</title>
    <meta name="description" content="ranjan's blog" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, 
            TeX: {equationNumbers: {autoNumber: "AMS"}}
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
    <!-- Customisation  -->
    <link rel="stylesheet" type="text/css" href="../assets/css/main.css" />

</head>
<body class="post-template">

    <header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        <a class="back-button icon-arrow-left" href="../../blog.html">Home</a>
    </nav>
</header>

<main class="content" role="main">

    <article class="post">

        <header class="post-header">
            <h1 class="post-title">Tensor differentiation</h1>
            <section class="post-meta">                
                <time class="post-date" datetime="2016-04-28">28 Apr 2016</time>
                    on Tensor Calculus
            </section>
        </header>




<section class="post-content">
        <p>
  Before going into Machine learning, one of the major concepts is needed  tensor calculus. To be more specific we need to know how to  differentiate   a scaler/vector/matrix with respect to  vector/matrix.
        In this blog I  am going discuss that will be needed in to understand  Deep learning.
	</p>

	<p>P.S Please don't skip this blog if you don't love mathematics because I will not go deep into mathematics. </p>


$
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{| #1 |}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\X}{\mathbf{x}}
\newcommand{\A}{\mathbf{a}}
\newcommand{\B}{\mathbf{b}}
\newcommand{\Z}{\mathbf{z}}

\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\NL}{\linebreak{NL}}
\newcommand{\*}{\times}
\newcommand{\pd}{\partial}
$


<h3>Differentiation of scalar with respect to vector/Matrix</h3>

<p> Let the  function is  $f : \R^n \longrightarrow \R$, 
The function is taking a input of  n  scalar variable(real value) and output a single scalar  real value.	<br/>
let the n variable of the function is $x_1,x_2,... x_n$. 
<br/>
<p>
So, $\mathbf {x}=\begin{bmatrix} x_1\\x_2\\x_3\\ ..\\.. \\ x_n\end{bmatrix} $  ; $ {\pd f(\X) \over \pd \X } =  \begin{bmatrix} \pd f(\X) \over \pd x_1 \\ \pd f(\X) \over \pd x_2 \\ ..\\ .. \\ \pd f(\X) \over \pd x_n   \end{bmatrix} $
</p>

$\X$ is a vector of length n. We can represent it as $n\* 1$ matrix.
Derivative of $f(\X)$  with respect to  $\X$, i.e $\pd f(\X) \over\pd \X$  will be a vector of size $n\*1$
when you do derivative  of a scalar with respect to vector the output  is always a vector as the size of vector by which you are doing derivative.
<br/>

<p>$Note:$ Similarly when you do derivative  of a  scalar with respect to matrix the output will always be a matrix as the size of the matrix by which you are doing derivative. You can visualize all this if you have done course in linear algebra Matrix calculus. 
</p>


<p>
First let us see the Matrix  algebra which will help us to workout in practical  <br />
Let $ \A,\X,\B $ is a vector of size in the form  $n\* 1$ . $A,X,C$ is matrix of size in the form $n\* m$.<br/>
 Note that"In the form" doesn't  mean exactly, like  in $\A^TX\B$, $\A$ may be of size  $n_1\* 1$ and $\B$ of size $n_2\* 1$ , as  $\A^TX\B$  is scalar and matrix multiplication property must hold so size of $X$  is $n_1\* n_2$ .
So $\X^Ta ,&nbsp \A^T\X,&nbsp \X^T\A\X,$ &nbsp $ \A^TX\B,&nbsp  \A^TX^T\B ,&nbsp \A^TX^TCX\B,&nbsp (X\B+\B)^TC(X\A+\B)$  are all scalar.
</p>

<h5>Scalar with respect to Vector </h5>
<ul>
<li>${\pd \A^T\X \over \pd \X} =\A$ &nbsp;&nbsp; as  $\A^T\X=\X^T\A$=scalar &nbsp;&nbsp; so&nbsp; ${\pd \X^T\A \over \pd \X} =\A$  </li>
<li>${\pd \X^T A \X \over \pd X} = (A+A^T) \X $  </li>
Note: $\X^TA\X$ is quadratic form of equation. By using this two we can do derivative of any scalar in matrix form
</ul>
<h5>Scalar with respect to Matrix </h5>
<ul>

<li>${\pd \A^TX\B  \over \pd X} = \A\B^T $  </li>
<li>${\pd  (\A^TX^T\B)   \over \pd X} = {\pd  (\A^TX^T\B)^T  \over \pd X} ={\pd  ((X^T\B)^T\A)  \over \pd X} = {\pd  (\B^TX\A)  \over \pd X}= \B\A^T  $</li>
as $\A^TX^T\B$ is scalar so $\A^TX^T\B=(\A^TX^T\B)^T $
<li>${\pd \A^TX^TCX\B  \over \pd X} = C^TX \A \B^T +CX \B a^T$  </li>
<li>${\pd (X\A+\B)^TC(X\A+\B)  \over \pd X} = {\pd ((X\A)^T+\B^T)C(X\A+\B)  \over \pd X}={\pd (\A^TX^T+\B^T)(CX\A+C\B))  \over \pd X}$
 $={{\pd \A^TX^TCX\A  \over \pd X} + {\pd \A^TX^TC\B  \over \pd X} + {\pd \B^TCX\A  \over \pd X} +{\pd \B^TC\B  \over \pd X }}= {C^TX \A \A^T +CX \A \A^T+C\B\A^T+ (\B^TC)^T \A^T +0 } $
$ = { (C^TX\A + C\B +CX \A +C^T\B)\A^T} = {(C+ C^T)(X\A+\B)\A^T } $
</li>
</ul>



<h3>Vector/matrix with respect to vector </h3>
<p>
When you  derivative a vector  with respect to  a vector out come will be a matrix. For example if  $\X$ has shape of  $n\* 1$ and $\Z$ has shape of  $m\* 1$
then $ {\pd \X^T \over \pd \Z}$ is a matrix of shape $m\*n$.<p>
<p>
Trick to remember(No mathematical logic): Derivative outcome shape is like multiplying to matrix.for the above example $\Z\X^T$ i.e  $m\* 1$  to $1\* n$ so the output shape is $m\*n$. 

<h5> Vector with respect to vector </h5>
<ul>
<li> ${\pd \X^T \over \pd \X}= I $</li>
<li> ${\pd \X^TA \over \pd \X}= A $</li>
<li> $ {\pd (A\X) \over \pd \Z}= {  A {\pd \X \over \pd \Z}} $ </li>
</ul>

<h5> Matrix with respect to vector </h5>
<ul>
<li> ${ \pd X^{-1}\over \pd \Z } ={ X^{-1}{\pd X\over \pd \Z }X^{-1}} $ </li>
<li> ${\pd AXB \over \pd \Z}= A{\pd X \over \pd \Z}B $</li>
<li> ${\pd XY \over \pd \Z}= {X{\pd Y \over \pd \Z} + {\pd X \over \pd \Z}Y} $</li>
</ul>

</p>







<h3>Further reading</h3>
<p>Now you will be able to understand the  problems which are formulated with vector/matrix and optimized. For more reading about these topics in more detail(not in shortcut ;) ) here I have given some resource.
<ul>
  <li>
  <a href="../assets/books/imm3274.pdf">The Matrix CookBook</a> </li>

</ul>

</p>

</section>




   <footer class="post-footer">
            <!-- If we want to display author's name and bio -->
            
                <figure class="author-image">
                    <a class="img" href="../../blog1.html" style="background-image: url(../assets/images/mars.jpg)">
                    <span class="hidden">Ranjan's Picture</span></a>
                </figure>
                <section class="author">
                    <!-- Author Name -->
                    <!--<h4> Stephen Tu </h4> -->
                    <!-- Author Bio -->
                    <!--
                    <p> 
                        Here goes the author description. You might want to place some links too in here
                    </p>
                    -->
                </section>                
           
 
            
     </footer>
</article>





</main>

    <footer class="site-footer clearfix">
      <section class="copyright">
        <a href="../../blog1.html">ranjan's blog</a> 
    </footer>
    
    <script type="text/javascript" src="../assets/js/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="../assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="../assets/js/index.js"></script>

</body>
</html>
